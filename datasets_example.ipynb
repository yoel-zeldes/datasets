{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll demonstrate how to use `tf.data.Dataset` to implement Curriculum Learning used in [Language Generation with Recurrent Generative Adversarial Networks without Pre-training](https://arxiv.org/pdf/1706.01399.pdf):\n",
    "\n",
    "> **Curriculum Learning (CL)**: In this extension,\n",
    "we start by training on short sequences and then\n",
    "slowly increase sequence length. In the first training\n",
    "stage, the generator G generates sequences of\n",
    "length 1, and the discriminator D receives real and\n",
    "generated sequences of length 1 as input. Then,\n",
    "the generator generates sequences of length 2 and\n",
    "the discriminator receives sequences of length 2.\n",
    "We increase sequence length in this manner until\n",
    "the maximum length of 32 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoel.z/projects/kaggle/kaggle-env/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we're going to use `tf.data.Dataset.from_generator` which allows us to use a simple python generator to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_filename = './datasets example.ipynb'\n",
    "with open(dataset_filename, 'r') as f:\n",
    "    dataset_text = '\\n'.join(f.readlines())\n",
    "    \n",
    "    \n",
    "def generate_generator(length):\n",
    "    '''\n",
    "    In Curriculum Learning we use multiple phases - each phase generates sequences of different length.\n",
    "    This function returns a function that when called returns a generator.\n",
    "    The generator can generate sequences of the given length.\n",
    "    Note that the returned function is exactly what's expected by tf.data.Dataset.from_generator.\n",
    "    '''\n",
    "    def generate_text():\n",
    "        start_index = 0\n",
    "        while start_index < len(dataset_text) - length:\n",
    "            yield dataset_text[start_index:start_index + length]\n",
    "            start_index += 1\n",
    "    return generate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_generator` is vanilla python code - no tensorflow involved.\n",
    "\n",
    "We can easily see what it gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\n",
      " \"cells\": [\n",
      "\n",
      "  {\n",
      "\n",
      "   \"cell_type\": \"markdown\",\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = generate_generator(length=50)()\n",
    "print generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 32\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    # create a dataset out of a generator that generates sequences with `length` charachters\n",
    "    tf.data.Dataset.from_generator(generate_generator(length), tf.string)\n",
    "    \n",
    "    # map the characters into numbers\n",
    "    .map(lambda t: tf.decode_raw(t, tf.int8))\n",
    "    \n",
    "    # shuffle the data (should be done only for training dataset)\n",
    "    .shuffle(buffer_size=1000, reshuffle_each_iteration=True)\n",
    "    \n",
    "    # train EPOCHS epochs\n",
    "    .repeat(EPOCHS)\n",
    "    \n",
    "    # use batch gradient descent\n",
    "    .batch(BATCH_SIZE)\n",
    "    \n",
    "    # create a dataset for every phase in the Curriculum Learning\n",
    "    for length in range(1, MAX_LENGTH + 1)\n",
    "]\n",
    "\n",
    "# create a handle which will choose which dataset to use (i.e. the current phase)\n",
    "handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "\n",
    "# create the iterator which will be used by the model\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    # the iterator will use the underlying iterator referenced by handle.\n",
    "    # remember that handle is a placeholder, so in runtime we'll decide which actual dataset to use.\n",
    "    string_handle=handle,\n",
    "    \n",
    "    # we must tell the iterator what types to expect.\n",
    "    # all the datasets have the same types.\n",
    "    output_types=datasets[0].output_types\n",
    ")\n",
    "\n",
    "# create a tensor that when evaluated will return the\n",
    "# next element of the dataset (whichever dataset the handle chooses)\n",
    "value = iterator.get_next()\n",
    "\n",
    "# for every dataset create an iterator\n",
    "iterators = [dataset.make_one_shot_iterator() for dataset in datasets]\n",
    "\n",
    "# and out of the iterators create handles so we can use them as a source to the handle created above\n",
    "handles = sess.run([iterator.string_handle() for iterator in iterators])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we trained for a while, and we reached to the last phase of the Curriculum Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase = MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek into `value` to see what's going to be fed to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example #0:\n",
      "-----------\n",
      "[100 105 115  99 114 105 109 105 110  97 116 111 114  32 114 101  99 101\n",
      " 105 118 101 115  32 115 101 113 117 101 110  99 101 115]\n",
      "discriminator receives sequences\n",
      "\n",
      "\n",
      "\n",
      "example #1:\n",
      "-----------\n",
      "[ 32  67 117 114 114 105  99 117 108 117 109  32  76 101  97 114 110 105\n",
      " 110 103  32 117 115 101 100  32 105 110  32  91  76  97]\n",
      " Curriculum Learning used in [La\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(sess.run(value, {handle: handles[phase - 1]})):\n",
    "    print 'example #{}:'.format(i)\n",
    "    print '-----------'\n",
    "    print example\n",
    "    print ''.join(map(chr, example))\n",
    "    print '\\n\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
